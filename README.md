# Multimodal-Human-State-Detection-System
A multimodal human state detection system that combines human activity recognition using smartphone sensors, voice emotion analysis using audio features, and face emotion detection using deep learning. The system fuses multiple modalities to infer overall human emotional and physical state in real time.
# Multimodal Human State Detection System

This project detects a personâ€™s overall state by combining three modalities: 
Human Activity Recognition (HAR) using sensor data, Voice Emotion Detection 
using audio features, and Face Emotion Detection using a webcam. The system 
fuses all outputs to determine states like Calm, Happy, Sad, Angry, or High Energy.

## ğŸ”§ Technologies Used
- Python 3.11
- Scikit-learn (Random Forest)
- Librosa (Audio Processing)
- DeepFace (Face Emotion)
- OpenCV
- NumPy, Pandas
- Tkinter (UI)

## ğŸ“‚ Project Modules
- `har_train.py` â€“ Train HAR model  
- `har_model.py` â€“ Save trained HAR model  
- `voice_emotion.py` â€“ Voice emotion detection  
- `face_emotion.py` â€“ Face emotion detection using webcam  
- `multimodal_system.py` â€“ Fusion logic  
- `multimodal_ui.py` â€“ User Interface  

## âš™ï¸ How It Works
1. Detects body activity from sensor data  
2. Records and analyzes voice emotion  
3. Detects facial emotion via webcam  
4. Combines all results to predict final human state  

## ğŸ¯ Applications
- Mental health monitoring  
- Smart surveillance  
- Humanâ€“Computer Interaction  
- Healthcare systems  

## ğŸš€ Future Scope
- Deep learningâ€“based fusion  
- Mobile app integration  
- Cloud deployment  
- Real-time dashboards  


